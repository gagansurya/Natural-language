{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1gvQn5xoZD6v-0Sl_SbFdcyDOfAyPb5BI","timestamp":1678989287039},{"file_id":"1XIe2_q4t2W1Be2Dcwa7zXfDLpyTWbBgP","timestamp":1678989137292}],"authorship_tag":"ABX9TyPSAUssN4Kp9DIDenoujJE+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4b573MsyjH3u"},"outputs":[],"source":["#general packages for data manipulation\n","#import os\n","import pandas as pd\n","import numpy as np\n","#visualizations\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","#consistent sized plot \n","from pylab import rcParams\n","rcParams['figure.figsize']=12,5\n","rcParams['axes.labelsize']=12\n","rcParams['xtick.labelsize']=12\n","rcParams['ytick.labelsize']=12\n","#handle the warnings in the code\n","import warnings\n","warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n","warnings.filterwarnings(action='ignore',category=FutureWarning)\n","#text preprocessing libraries\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import sent_tokenize\n","from nltk.tokenize import WordPunctTokenizer\n","from nltk.tokenize import TweetTokenizer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import PorterStemmer\n","#import texthero\n","#import texthero as hero\n","#regular expressions\n","import re\n","#display pandas dataframe columns \n","pd.options.display.max_columns = None"]},{"cell_type":"code","source":[" # importing dataset\n"," tweet=pd.read_csv('/content/TwitterHate.csv')"],"metadata":{"id":"qkm0nToUjq0W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tweet.head()"],"metadata":{"id":"5nlaovb5mQiJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tweet.tail()"],"metadata":{"id":"TVotpOCHmUj5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#get rid of the identifier number of the tweet\n","tweet.drop('id',axis=1,inplace=True)"],"metadata":{"id":"ZnSQ-e8Um2ph"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tweet.head()"],"metadata":{"id":"TeABzmuUyVCJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#view one of the tweets randomly \n","random = np.random.randint(0,len(tweet))\n","print(random)\n","tweet.iloc[random]['tweet']"],"metadata":{"id":"Tl0JFJCjzSNe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create a copy of the original data to work with \n","df = tweet.copy()"],"metadata":{"id":"C6ocAkEvzf5d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Text Cleaning"],"metadata":{"id":"4tbHcR-c1Mp3"}},{"cell_type":"code","source":[],"metadata":{"id":"DvggXC1q1PzF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Handle Diacritics using text normalization"],"metadata":{"id":"__E1tfqp1U-U"}},{"cell_type":"code","source":["def simplify(text):\n","    '''Function to handle the diacritics in the text'''\n","    import unicodedata\n","    try:\n","        text = unicode(text, 'utf-8')\n","    except NameError:\n","        pass\n","    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n","    return str(text)"],"metadata":{"id":"7lXVg1281W1s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['tweet'] = df['tweet'].apply(simplify)"],"metadata":{"id":"9WQ998Ip2g8k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Remove user handles"],"metadata":{"id":"ZTvM95sI3mkT"}},{"cell_type":"code","source":["#test on a sample string\n","sample = \"and @user1 i would like you to discuss with @user2 and then with @username3\"\n","pattern = re.compile(r'@\\w+')\n","re.findall(pattern,sample)"],"metadata":{"id":"Btv7vjOU3oay"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#remove all the user handles --> strings starting with @Remove the urls¶\n","df['tweet'].replace(r'@\\w+','',regex=True,inplace=True)"],"metadata":{"id":"WRNY9YJ04aA3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Remove the urls¶\n"],"metadata":{"id":"o01uVC_64nWB"}},{"cell_type":"code","source":["#test on a sample \n","sample = \"https://www.machinelearing.com prakhar and https://www.simple.com\"\n","pattern = re.compile(r'http\\S+')\n","re.findall(pattern,sample)"],"metadata":{"id":"2AF9hzBC4o6B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['tweet'].replace(r'http\\S+','',regex=True,inplace=True)"],"metadata":{"id":"RWOICOmO41K-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tokenize using tweet tokenizer"],"metadata":{"id":"Fgn7ct6H5Sl-"}},{"cell_type":"code","source":["#test on a sample text\n","sample = 'wonderfl :-)  when are you coming for #party'\n","tweet_tokenize = TweetTokenizer(preserve_case=True)\n","tweet_tokenize.tokenize(sample)"],"metadata":{"id":"NzM-oJHw5T1Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#tokenize the tweets in the dataframe using TweetTokenizer\n","tokenizer = TweetTokenizer(preserve_case=True)\n","df['tweet'] = df['tweet'].apply(tokenizer.tokenize)"],"metadata":{"id":"JWYOzHRe5biF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#view the tokenized tweets\n","df.head(3)"],"metadata":{"id":"Gsde0gfd6grr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Remove Stopwords¶\n","Append more words to be removed from the text - example rt and amp which occur very frequently"],"metadata":{"id":"7cCaN5mT7JgG"}},{"cell_type":"code","source":["nltk.download('stopwords')"],"metadata":{"id":"xsiccWyn76KZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stop_words = stopwords.words('english')\n","\n","#add additional stop words to be removed from the text\n","additional_list = ['amp','rt','u',\"can't\",'ur']\n","\n","for words in additional_list:\n","    stop_words.append(words)"],"metadata":{"id":"KGfWQRuu7T0U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stop_words[-10:]"],"metadata":{"id":"oslzdS4D7LNe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#remove stop words\n","def remove_stopwords(text):\n","    '''Function to remove the stop words from the text corpus'''\n","    clean_text = [word for word in text if not word in stop_words]\n","    return clean_text    "],"metadata":{"id":"WRSYaE718JUJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#remove the stop words from the tweets\n","df['tweet'] = df['tweet'].apply(remove_stopwords)"],"metadata":{"id":"y1NWrZnq84a_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['tweet'].head()"],"metadata":{"id":"eYw8RMcs86cf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Spelling corrections"],"metadata":{"id":"cqMrseTV9O6T"}},{"cell_type":"code","source":["#apply spelling correction on a sample text\n","from textblob import TextBlob\n","sample = 'amazng man you did it finallyy'\n","txtblob = TextBlob(sample)\n","corrected_text = txtblob.correct()\n","print(corrected_text)"],"metadata":{"id":"XxUzy2z19V5x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#textblob expect a string to be passed and not a list of strings\n","from textblob import TextBlob\n","\n","def spell_check(text):\n","    '''Function to do spelling correction using '''\n","    txtblob = TextBlob(text)\n","    corrected_text = txtblob.correct()\n","    return corrected_text"],"metadata":{"id":"8pkCRnhs9Qkq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qcShZmwl9kS_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*Remove* # symbols while retaining the text"],"metadata":{"id":"b1sN1ZzX9oZh"}},{"cell_type":"code","source":["#try tremoving # symbols from a sample text\n","sample = '#winner #machine i am learning'\n","pattern = re.compile(r'#')\n","re.sub(pattern,'',sample)"],"metadata":{"id":"FZKlNbf29tVL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_hashsymbols(text):\n","    '''Function to remove the hashtag symbol from the text'''\n","    pattern = re.compile(r'#')\n","    text = ' '.join(text)\n","    clean_text = re.sub(pattern,'',text)\n","    return tokenizer.tokenize(clean_text) "],"metadata":{"id":"EkZPutMv9pe8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['tweet'] = df['tweet'].apply(remove_hashsymbols)"],"metadata":{"id":"dph4MvS6EMh2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head(3)"],"metadata":{"id":"aU5G9OMtEQvG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Remove single and double length characters¶"],"metadata":{"id":"QoST_RMuFSZH"}},{"cell_type":"code","source":["def rem_shortwords(text):\n","    '''Function to remove the short words of length 1 and 2 characters'''\n","    '''Arguments: \n","       text: string\n","       returns: string without containing words of length 1 and 2'''\n","    lengths = [1,2]\n","    new_text = ' '.join(text)\n","    for word in text:\n","        text = [word for word in tokenizer.tokenize(new_text) if not len(word) in lengths]\n","        \n","    return new_text    "],"metadata":{"id":"yPf9zF7zFUMw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['tweet'] = df['tweet'].apply(rem_shortwords)"],"metadata":{"id":"gD1C-vYNFZtc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head(2)"],"metadata":{"id":"zafsLOP4FfuM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['tweet'] = df['tweet'].apply(tokenizer.tokenize)"],"metadata":{"id":"pfgJQXzTGmzC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head(3)"],"metadata":{"id":"tUexsbQoG0MC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Remove digits"],"metadata":{"id":"BvQBJ5iwG4zy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Remove digits"],"metadata":{"id":"O4vwmLC8HQ2c"}},{"cell_type":"code","source":["def rem_digits(text):\n","    '''Function to remove the digits from the list of strings'''\n","    no_digits = []\n","    for word in text:\n","        no_digits.append(re.sub(r'\\d','',word))\n","    return ' '.join(no_digits) "],"metadata":{"id":"Fi0OW5q7HS6W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['tweet'] = df['tweet'].apply(rem_digits)"],"metadata":{"id":"QlaNPDVbHXhp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['tweet'] = df['tweet'].apply(tokenizer.tokenize)"],"metadata":{"id":"MmW7dfE5IMGg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"shn4y47KISKA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3Col8wr3IVf4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Remove special characters"],"metadata":{"id":"gzhemzJHIwRb"}},{"cell_type":"code","source":["def rem_nonalpha(text):\n","    '''Function to remove the non-alphanumeric characters from the text'''\n","    text = [word for word in text if word.isalpha()]\n","    return text"],"metadata":{"id":"HM5DmpxLIxlK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#remove the non alpha numeric characters from the tweet tokens\n","df['tweet'] = df['tweet'].apply(rem_nonalpha)"],"metadata":{"id":"yb1f_TdzI0vA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VXTG4rWqJxw2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"_qa65xHfJ_yd"}},{"cell_type":"markdown","source":["## Exploratory Data Analysis - Broad Approach"],"metadata":{"id":"PJY79HVEJ2SJ"}},{"cell_type":"markdown","source":["Check for data balance"],"metadata":{"id":"mJ4KRvPpJ8Ex"}},{"cell_type":"code","source":["#plot of the count of hate and non hate tweet\n","sns.countplot(df['label'])\n","plt.title('Count of Hate vs Non Hate Tweet')\n","plt.grid()\n","plt.show()\n"],"metadata":{"id":"lM7lSYy7J9do"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There are more non hatespeeches than the hatespeech in the dataset\n","\n","\n"],"metadata":{"id":"viGtEqCJKhDT"}},{"cell_type":"markdown","source":["Check out the top terms in the tweets¶\n"],"metadata":{"id":"tpvtw5pqKvoa"}},{"cell_type":"code","source":["from collections import Counter\n","results = Counter()\n","df['tweet'].apply(results.update)\n","#print the top 10 most common terms in the tweet \n","print(results.most_common(10))"],"metadata":{"id":"WQo3eismKw2x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#plot the cumulative frequency of the top 10 most common tokens \n","frequency = nltk.FreqDist(results)\n","plt.title('Top 10 Most Common Terms')\n","frequency.plot(10,cumulative=True)\n","plt.show()"],"metadata":{"id":"rc0TnjaNKiZP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#plot the frequency of the top 10 most common tokens \n","frequency = nltk.FreqDist(results)\n","plt.title('Top 10 Most Common Terms')\n","frequency.plot(10,cumulative=False)\n","plt.show()"],"metadata":{"id":"TCur1ZsLLh1E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Love is the most frequently used word followed by day, happy etc. This is expected as there are more non hate tweets than hate tweets in the dataset"],"metadata":{"id":"87kihD-hMEnW"}},{"cell_type":"markdown","source":["# Predictive Modeling"],"metadata":{"id":"fe2wyuLtM1bU"}},{"cell_type":"markdown","source":["Data Formatting for Predictive Modeling¶\n"],"metadata":{"id":"HHt4uYkXM9Wt"}},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"zUmI9nlTMFwW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#check for the null values\n","df.isnull().sum()"],"metadata":{"id":"hux7-NjPNFey"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#join the tokens back to form the string\n","df['tweet'] = df['tweet'].apply(lambda x: ' '.join(x))"],"metadata":{"id":"bVxCMrX9NMFx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#check the top rows\n","df.head(3)"],"metadata":{"id":"0ziK1mXINRE5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#split the data into input X and output y\n","X = df['tweet']\n","y = df['label']"],"metadata":{"id":"_xrWiTLHNmYl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#split the data \n","from sklearn.model_selection import train_test_split\n","seed = 51\n","test_size = 0.2 #20% of the data in the \n","X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=seed,stratify=df['label'])\n","print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"],"metadata":{"id":"GBSXz7UfN3-w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Use tf-idf as a feature to get into the vector space model"],"metadata":{"id":"k_I_cLDlOQaz"}},{"cell_type":"code","source":["#import tfidf vectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer"],"metadata":{"id":"6udtILbvOSYz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#instantiate the vectorizer \n","vectorizer = TfidfVectorizer(max_features=5000)"],"metadata":{"id":"AVOXVfgdPKpF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"3L8OKTvkOPcD"}},{"cell_type":"code","source":["#fit on the training data\n","X_train = vectorizer.fit_transform(X_train)\n","#transform the test data\n","X_test = vectorizer.transform(X_test)"],"metadata":{"id":"BoeYnWUbPSYt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#check the shape\n","X_train.shape, X_test.shape"],"metadata":{"id":"BfFRpToKPVzG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Model building: Ordinary Logistic Regression¶\n"],"metadata":{"id":"sWLozOqGPqKl"}},{"cell_type":"code","source":["#import the models\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import MultinomialNB"],"metadata":{"id":"ougc0h5JPrpZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#instantiate the models with default hyper-parameters\n","clf = LogisticRegression()\n","clf.fit(X_train,y_train)\n","train_predictions = clf.predict(X_train)\n","test_predictions = clf.predict(X_test)"],"metadata":{"id":"DS7jrA5DPw7m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Model evaluation"],"metadata":{"id":"qknADQtKQHjI"}},{"cell_type":"code","source":["#import the metrics\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix"],"metadata":{"id":"9lhlytiAQJ-R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#get the model accuracy on the training and the test set\n","print('Accuracy Score on training set %.5f' %accuracy_score(y_train,train_predictions))\n","print('Accuracy Score on test set %.5f' %accuracy_score(y_test,test_predictions))"],"metadata":{"id":"9HSIUc1aQcn1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Accuracy is never a good metric for an imbalanced dataset as in this case. This can be highighted using the f1 score. A low f1-score for a label indicate poor performance of the model."],"metadata":{"id":"fAREMl2cQkZS"}},{"cell_type":"code","source":["print('Classification Report Training set')\n","print('\\n')\n","print(classification_report(y_train,train_predictions))"],"metadata":{"id":"FBKnGbU-QmAY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Classification Report Testing set')\n","print('\\n')\n","print(classification_report(y_test,test_predictions))"],"metadata":{"id":"3teXBqngQsa1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["he model's f1-score is low for label 1 which indicates the hate text in the twitter"],"metadata":{"id":"BvRthToBQ3um"}},{"cell_type":"markdown","source":["\n","\n","\n","#Weighted Logistic Regression Or Cost Sensitive Logistic Regression¶"],"metadata":{"id":"zFeAoLPHRAH2"}},{"cell_type":"code","source":["df['label'].value_counts()"],"metadata":{"id":"xe3fZcCVRCO2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The minority to majority class ratio is 1:13\n","\n"],"metadata":{"id":"_m799EEaR3Rn"}},{"cell_type":"code","source":["#define the weight of the class labels using inverse ratio\n","weights = {0:1.0,1:13.0}\n","\n","#instantiate the logistic regression model and account for the weights to be applied for model coefficients update magnitude\n","clf = LogisticRegression(solver='lbfgs',class_weight=weights)\n","\n","#fit and predict\n","clf.fit(X_train,y_train)\n","train_predictions = clf.predict(X_train)\n","test_predictions = clf.predict(X_test)\n","\n","#classification report\n","print('Classification Report Training set')\n","print('------------------------------------')\n","print('\\n')\n","print(classification_report(y_train,train_predictions))\n","print('\\n')\n","\n","print('Classification Report Testing set')\n","print('------------------------------------')\n","print('\\n')\n","print(classification_report(y_test,test_predictions))"],"metadata":{"id":"ljTqo50kR4nf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The f1 score of both the training and testing set has improved compared to the plain vanilla Logistic Regression model. There is still more opportunity to improve the score using better models or even handling the data imbalance by adding synthetic data"],"metadata":{"id":"OzsGqZe-TfDv"}},{"cell_type":"markdown","source":[],"metadata":{"id":"go2MioQATm6G"}},{"cell_type":"markdown","source":["Regularization and Hyperparameter tuning:"],"metadata":{"id":"gZxqesH_Tn1s"}},{"cell_type":"code","source":["#import the required libraries for grid search\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import cross_val_score"],"metadata":{"id":"AOUowpEnTpJM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define search space\n","from scipy.stats import loguniform\n","space = dict()\n","space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n","space['penalty'] = ['l1', 'l2', 'elasticnet']\n","space['C'] = loguniform(1e-5, 100)"],"metadata":{"id":"I5YguGeoT0vu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#check the search space \n","print(space)"],"metadata":{"id":"XQ7zakLIT13Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fine tuned Model with Balanced Class Weights - !!!!!!!"],"metadata":{"id":"GArp1SsxT9JD"}},{"cell_type":"code","source":["#define the model with balanced class weights\n","weights = {0:1.0,1:1.0}\n","clf = LogisticRegression(class_weight=weights)"],"metadata":{"id":"vmT9qK_vT-Uy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#define the number of folds \n","folds = StratifiedKFold(n_splits=4,shuffle=True,random_state=seed)"],"metadata":{"id":"0XBVLGAeUEkg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define search\n","grid_search = RandomizedSearchCV(estimator=clf,param_distributions=space, n_iter=100, scoring='recall',\n","                            n_jobs=-1, cv=folds, random_state=seed)\n","#fit grid search on the train data\n","grid_result = grid_search.fit(X_train,y_train)"],"metadata":{"id":"bIpVNYtFV441"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#retrieve the best model \n","grid_result.best_estimator_"],"metadata":{"id":"N7rzIyXnWVvM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#instantiate the best model\n","clf = LogisticRegression(C=23.871926754399514,penalty='l1',solver='liblinear',class_weight=weights)"],"metadata":{"id":"5HcUfxMjWeXd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#fit and predict\n","clf.fit(X_train,y_train)\n","train_predictions = clf.predict(X_train)\n","test_predictions = clf.predict(X_test)\n","\n","#classification report\n","print('Classification Report Training set')\n","print('------------------------------------')\n","print('\\n')\n","print(classification_report(y_train,train_predictions))\n","print('\\n')\n","\n","print('Classification Report Testing set')\n","print('------------------------------------')\n","print('\\n')\n","print(classification_report(y_test,test_predictions))"],"metadata":{"id":"iqWZ1-RMWl68"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"phpg4EwLW-Kx"}},{"cell_type":"markdown","source":["Fine tuned model with class weights proportional to the class imbalance"],"metadata":{"id":"ofheLr3hXTev"}},{"cell_type":"code","source":["#use the class weights to handle the imbalance in the labels\n","weights = {0:1.0,1:13}\n","\n","clf = LogisticRegression(class_weight=weights)\n","#define the number of folds \n","folds = StratifiedKFold(n_splits=4,random_state=seed,shuffle=True)\n","# define search\n","grid_search = RandomizedSearchCV(estimator=clf,param_distributions=space, n_iter=100, scoring='recall',\n","                            n_jobs=-1, cv=folds, random_state=seed)\n","#fit grid search on the train data\n","grid_result = grid_search.fit(X_train,y_train)\n","\n","#retrieve the best model \n","grid_result.best_estimator_"],"metadata":{"id":"DdMhi6_OXVNi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#instantiate the best model\n","clf = LogisticRegression(C=0.16731783677034165,penalty='l2',solver='liblinear',class_weight=weights)\n","\n","#fit and predict\n","clf.fit(X_train,y_train)\n","train_predictions = clf.predict(X_train)\n","test_predictions = clf.predict(X_test)\n","\n","#classification report\n","print('Classification Report Training set')\n","print('------------------------------------')\n","print('\\n')\n","print(classification_report(y_train,train_predictions))\n","print('\\n')\n","\n","print('Classification Report Testing set')\n","print('------------------------------------')\n","print('\\n')\n","print(classification_report(y_test,test_predictions))"],"metadata":{"id":"aTua6rbDXeA7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install cartopy\n","import cartopy"],"metadata":{"id":"XXc9hJe8ZUe5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --upgrade scikit-learn"],"metadata":{"id":"7Azbn7DLaBKQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!apt-get -qq install -y libarchive-dev && pip install -U libarchive\n","import libarchive"],"metadata":{"id":"VhOhV9vtbm1H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install cartopy\n","import cartopy"],"metadata":{"id":"7mFr3lFOb60D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install matplotlib-venn"],"metadata":{"id":"gr0bKk2unpIM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!apt-get -qq install -y libfluidsynth1"],"metadata":{"id":"wLFbeMAGny7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --upgrade scikit-learn"],"metadata":{"id":"EKw-oGp6pklv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python3 -m pip show scikit-learn"],"metadata":{"id":"hscp8Jnzr7hN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install cartopy\n","import cartopy"],"metadata":{"id":"h9h2R-stziRb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip update -c conda-forge scikit-learn"],"metadata":{"id":"ZTvZX0ns13IQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -U scikit-lear"],"metadata":{"id":"7n8bX7qO1smF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.utils.multiclass import unique_labels\n","from sklearn.metrics import confusion_matrix\n","import matplotlib.pyplot as plt\n","\n","plot_confusion_matrix(clf, X_test, y_test, cmap='summer')\n","plt.title('Confusion Matrix Test Set')\n","plt.show()"],"metadata":{"id":"6uH2fxMCn85S"},"execution_count":null,"outputs":[]}]}